{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(\n",
    "    openai_api_base=os.environ[\"CHATGPT_API_ENDPOINT\"],\n",
    "    openai_api_key=os.environ[\"OPENAI_API_KEY\"]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2 Chroma向量數據庫相似度搜索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#讀取PDF文件資料\n",
    "loaders = [\n",
    "    PyPDFLoader(\"./data/01.pdf\"),\n",
    "    PyPDFLoader(\"./data/02.pdf\"),\n",
    "    PyPDFLoader(\"./data/03.pdf\"),\n",
    "    PyPDFLoader(\"./data/04.pdf\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#將資料個別放入docs，以document格式儲存\n",
    "docs = []\n",
    "\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#分割資料為多個chunk，每個chunk大小為1000\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用openAI的embedding工具，將資料進行embedding\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    base_url=os.environ[\"EMBEDDINGS_BASE_URL\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "persist_directory = \"./db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#先將db中資料清空\n",
    "#!rm -rf ./db    #Linux指令\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# 檢查目標是否存在\n",
    "if os.path.exists('./db'):\n",
    "    # 删除目錄及其内容\n",
    "    shutil.rmtree('./db')\n",
    "    print(\"目錄已刪除\")\n",
    "else:\n",
    "    print(\"目錄不存在\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#建立向量資料庫\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#搜尋與問題相似度最高的資料\n",
    "question = \"有什麼西式美食推薦?\"\n",
    "\n",
    "docs_ss = vectordb.similarity_search(question, k=3)   #返回3筆\n",
    "docs_nmr = vectordb.max_marginal_relevance_search(question, k=2, fetch_k=3)   #先找3個，刪掉1個最相似的(留下2個)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs_nmr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_ss[0].page_content[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_nmr[0].page_content[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"有什麼景色優美的景點可以推薦?\"\n",
    "\n",
    "docs_ss = vectordb.similarity_search(\n",
    "    question,\n",
    "    k=3,\n",
    "    filter={\"source\": \"./data/03.pdf\"}   #針對指定檔案搜索\n",
    ")\n",
    "\n",
    "for d in docs_ss:\n",
    "    print(d.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用地端模型\n",
    "chat = Ollama(model=\"openchat:latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.self_query.base import SelfQueryRetriever \n",
    "from langchain.chains.query_constructor.base import AttributeInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#進行檢索資料設定\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"source\",\n",
    "        description=\"搜索的訊息來源於以下三個PDF文檔，他們分別是`./data/01.pdf`, `./data/02.pdf`, `./data/03.pdf`,`./data/04.pdf`\",\n",
    "        type=\"string\"\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"page\",\n",
    "        description=\"訊息來源的頁面\",\n",
    "        type=\"integer\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_content_description = \"這裡存放的是關於香港特色的旅遊勝地以及美食和特有文化紀錄\"\n",
    "\n",
    "#進行檢索設定\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm=chat,\n",
    "    vectorstore=vectordb,\n",
    "    document_contents=document_content_description,\n",
    "    metadata_field_info=metadata_field_info\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#進行檢索\n",
    "question = \"介紹一下香港特色美食?\"\n",
    "\n",
    "docs = retriever.invoke(question, k=5)\n",
    "\n",
    "for d in docs:\n",
    "    print(d.metadata)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.4 如何使用LLM摘要總結Chroma檢索訊息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#一般相似度搜索\n",
    "compressor = LLMChainExtractor.from_llm(chat)\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_retriever=vectordb.as_retriever(),\n",
    "    base_compressor=compressor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"介紹一下西貢的優美景色\"\n",
    "\n",
    "compressed_docs = compression_retriever.invoke(question)\n",
    "\n",
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n\\n{'-'*60}\".join([f\"\\n\\n第{i+1}個檢索:\\n\\n\" + d.page_content for i, d in enumerate(docs)])\n",
    "    )\n",
    "\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nmr搜索，避免有相同的\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vectordb.as_retriever(search_type=\"mmr\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"香港哪裡有最好吃的蛋塔? 如果有，請提供該店鋪的地址\"\n",
    "\n",
    "compressed_docs = compression_retriever.invoke(question)\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.5 如何初始化FAISS與RetrievalQA的使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "#讀取PDF文件資料\n",
    "loaders = [\n",
    "    PyPDFLoader(\"./data/01.pdf\"),\n",
    "    PyPDFLoader(\"./data/02.pdf\"),\n",
    "    PyPDFLoader(\"./data/03.pdf\"),\n",
    "    PyPDFLoader(\"./data/04.pdf\")\n",
    "]\n",
    "\n",
    "#將資料個別放入docs，以document格式儲存\n",
    "docs = []\n",
    "\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "#分割資料為多個chunk，每個chunk大小為1000\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "len(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "#使用openAI的embedding工具，將資料進行embedding\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    base_url=os.environ[\"EMBEDDINGS_BASE_URL\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "\n",
    "#使用地端模型\n",
    "chat = Ollama(model=\"openchat:latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"在香港有什麼美味的食物? 如果有，請給出該店鋪的地址\"\n",
    "\n",
    "docs = vectordb.similarity_search(query)\n",
    "docs[0].page_content[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chains: 把多個的任務 or LLM連接再一起\n",
    "#RetrievalQA = 檢索 + 回答問題\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RetrievalQA.from_chain_type(\n",
    "    retriever=retriever,\n",
    "    llm=chat,\n",
    "    chain_type=\"stuff\", #把檢索到的文檔，直接輸入LLM，不做任何處理\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model(\n",
    "    {\"query\": query},\n",
    "    return_only_outputs=True\n",
    ")\n",
    "print(\"回答:\", response[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.6 如何保存與加載FAISS並製表查看FAISS中的文檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "#讀取PDF文件資料\n",
    "loaders = [\n",
    "    PyPDFLoader(\"./data/01.pdf\"),\n",
    "    PyPDFLoader(\"./data/02.pdf\"),\n",
    "    PyPDFLoader(\"./data/03.pdf\"),\n",
    "    PyPDFLoader(\"./data/04.pdf\")\n",
    "]\n",
    "\n",
    "#將資料個別放入docs，以document格式儲存\n",
    "docs = []\n",
    "\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "#分割資料為多個chunk，每個chunk大小為1000\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "len(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "#使用openAI的embedding工具，將資料進行embedding\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    base_url=os.environ[\"EMBEDDINGS_BASE_URL\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "\n",
    "#使用地端模型\n",
    "chat = Ollama(model=\"openchat:latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "vectordb = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#先將db中資料清空\n",
    "#!rm -rf ./db    #Linux指令\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# 檢查目標是否存在\n",
    "if os.path.exists('./db'):\n",
    "    # 删除目錄及其内容\n",
    "    shutil.rmtree('./db')\n",
    "    print(\"目錄已刪除\")\n",
    "else:\n",
    "    print(\"目錄不存在\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = \"./db\"\n",
    "\n",
    "vectordb.save_local(persist_directory)\n",
    "\n",
    "new_db = FAISS.load_local(                \n",
    "    folder_path = persist_directory,\n",
    "    embeddings=embeddings,\n",
    "    allow_dangerous_deserialization=True    #將資料反序列化，以供讀取文檔\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看VectorDB的文檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_to_df(vectordb):\n",
    "    v_dict = vectordb.docstore._dict\n",
    "    data_rows = []\n",
    "    for k in v_dict.keys():\n",
    "        doc_name = v_dict[k].metadata[\"source\"].split(\"/\")[-1]\n",
    "        page_number = v_dict[k].metadata[\"page\"] + 1\n",
    "        content = v_dict[k].page_content\n",
    "        data_rows.append({\"chunk_id\": k, \"doc_name\": doc_name, \"page_number\": page_number, \"content\": content})\n",
    "\n",
    "    vector_df = pd.DataFrame(data_rows)\n",
    "    return vector_df\n",
    "\n",
    "def show_vectorStore(vectordb):\n",
    "    vector_df = store_to_df(vectordb)\n",
    "    display(vector_df)\n",
    "\n",
    "show_vectorStore(new_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.7 如何在FAISS中添加和刪除VectorDB中的文檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chains: 把多個的任務 or LLM連接再一起\n",
    "#RetrievalQA = 檢索 + 回答問題\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "#刪除對應PDF的文檔\n",
    "def delete_document(store, document_name):\n",
    "    vector_df = store_to_df(store)\n",
    "    chunks_list = vector_df.loc[vector_df[\"doc_name\"]==document_name][\"chunk_id\"].tolist()\n",
    "    store.delete(ids=chunks_list)\n",
    "\n",
    "#更新VectorDB, 同時更新我們的RetrievalQA, 即是答問系統\n",
    "def refresh_model(new_store):\n",
    "    retriever = new_store.as_retriever()\n",
    "    model = RetrievalQA.from_chain_type(\n",
    "        llm=chat,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        verbose=True\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_document(new_db, '02.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_vectorStore(new_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = refresh_model(new_db)\n",
    "\n",
    "query = \"泰昌餅家的地址是哪裡?\"\n",
    "\n",
    "response = model(\n",
    "    {\"query\": query},\n",
    "    return_only_outputs=True\n",
    ")\n",
    "\n",
    "print(\"我的回答是:\", response[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何添加文檔到VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_vectorStore(store, directory):\n",
    "\n",
    "    #讀取PDF文件資料\n",
    "    loader = PyPDFLoader(directory)\n",
    "\n",
    "    doc = loader.load()\n",
    "\n",
    "    #分割資料為多個chunk，每個chunk大小為1000\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    chunks = text_splitter.split_documents(doc)\n",
    "\n",
    "    extension = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "    store.merge_from(extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_vectorStore(new_db, \"./data/05.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_vectorStore(new_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 和昌飯店\n",
    "model = refresh_model(new_db)\n",
    "\n",
    "query = \"和昌飯店有什麼好吃的? 它的地址和電話是什麼?\"\n",
    "\n",
    "response = model(\n",
    "    {\"query\": query},\n",
    "    return_only_outputs=True\n",
    ")\n",
    "\n",
    "print(\"我的回答是:\", response[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
