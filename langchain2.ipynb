{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sk-08GBKw9V6YPjQbuOojfI1hSWNSXO5LNKqczpX0EtbLTKnFZq'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(\n",
    "    openai_api_base=os.environ[\"CHATGPT_API_ENDPOINT\"],\n",
    "    openai_api_key=os.environ[\"OPENAI_API_KEY\"]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 如何加載PDF和搜索網頁信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "##讀取PDF資料\n",
    "loader = PyPDFLoader(\"./data/ML-guide.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#確認頁數\n",
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MachineLearning-Lecture01  \n",
      "Instructor (Andrew Ng): Okay. Good morning. Welcome to CS229, the machine \n",
      "learning class. So what I wanna do today is just spend a little time going over the logistics \n",
      "of the class, and then we'll start to talk a bit about machine learning.  \n",
      "By way of introduction, my name's Andrew Ng and I'll be instructor for this class. And so \n",
      "I personally work in machine learning, and I've worked on it for about 15 years now, and \n",
      "I actually think that machine learning is the \n"
     ]
    }
   ],
   "source": [
    "#讀取第1頁前500字符訊息\n",
    "page = pages[0]\n",
    "print(page.page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': './data/ML-guide.pdf', 'page': 0}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#了解資料來源\n",
    "page.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#讀取網頁資料\n",
    "loader = WebBaseLoader(\"http://google.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google搜尋 圖片 地圖 Play YouTube 新聞 Gmail 雲端硬碟 更多 »網頁記錄 | 設定 | 登入 進階搜尋Google 提供：  English 廣告商業解決方案關於 GoogleGoogle.com.tw© 2024 - 隱私權 - 服務條款  \n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"searchParameters\":{\"q\":\"apple inc\",\"hl\":\"zh-tw\",\"type\":\"news\",\"engine\":\"google\"},\"news\":[{\"title\":\"Jim Cramer on Apple Inc. (AAPL): ‘It Was The Dumb Money That Was Getting Out, Because Warren Buffett Or One Of His Assistants Made Us Nervous’\",\"link\":\"https://finance.yahoo.com/news/jim-cramer-apple-inc-aapl-053136089.html\",\"snippet\":\"We recently published an article titled Jim Cramer's Bold Predictions About These 15 AI Stocks. In this article, we are going to take a look...\",\"date\":\"3 小時前\",\"source\":\"Yahoo Finance\",\"imageUrl\":\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRqs3Ew-dTK28r3XdWgkrPRm-JtIU5tkqmRPSuzlUE1blx3MPEQ9CcevfvDkg&s\",\"position\":1},{\"title\":\"Apple Inc. stock outperforms competitors on strong trading day\",\"link\":\"https://www.marketwatch.com/data-news/apple-inc-stock-outperforms-competitors-on-strong-trading-day-d3d380d6-9bde2aa95ecc\",\"snippet\":\"inched 0.32% higher to $259.02 Thursday, on what proved to be an all-around mixed trading session for the stock market, with the Dow Jones...\",\"date\":\"3 天前\",\"source\":\"MarketWatch\",\"imageUrl\":\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTlSkpFJdUdaRMlroj4TAaUo2_sceSyfqkCEeOx8vCL1l_fRoukfamsF97Wiw&s\",\"position\":2},{\"title\":\"Apple (AAPL) Ascends While Market Falls: Some Facts to Note\",\"link\":\"https://finance.yahoo.com/news/apple-aapl-ascends-while-market-224521758.html\",\"snippet\":\"Apple (AAPL) concluded the recent trading session at $259.02, signifying a +0.32% move from its prior day's close.\",\"date\":\"3 天前\",\"source\":\"Yahoo Finance\",\"imageUrl\":\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQbNq5g0NLVxh9_xkI5J874SP5oK-pvbUzIkGGIm9x_Wu6exz_XnZrkpKkq2A&s\",\"position\":3},{\"title\":\"Why do the Folks at APPLE Inc. think they can use my Business Name?\",\"link\":\"https://tapeop.com/blog/2024/12/29/why-do-folks-apple-inc-think-they-can-use-my-busin/\",\"snippet\":\"A reader just informed me that APPLE's recent Logic Pro X includes \\\"Drummer\\\", a virtual drummer plug-in. One of the virtual drummers is...\",\"date\":\"1 天前\",\"source\":\"Tape Op\",\"imageUrl\":\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRV_IEhvfFJFR-qsoI83nvjtpluSZOAnQDB-2I6eyGJn9jxSTAVJLU1s5Zt4g&s\",\"position\":4},{\"title\":\"Boston Financial Mangement LLC Lowers Position in Apple Inc. (NASDAQ:AAPL)\",\"link\":\"https://www.marketbeat.com/instant-alerts/boston-financial-mangement-llc-lowers-position-in-apple-inc-nasdaqaapl-2024-12-30/\",\"snippet\":\"Boston Financial Mangement LLC lessened its position in shares of Apple Inc. (NASDAQ:AAPL - Free Report) by 4.3% in the 3rd quarter,...\",\"date\":\"21 分鐘前\",\"source\":\"MarketBeat\",\"imageUrl\":\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcReZsLWjGcrl-gVnwRLLWUbLdLlBNNjwDc4gmV3tS_Ba-07Vn1M3l2RVw6K3g&s\",\"position\":5},{\"title\":\"Got an Apple computer for Christmas? Here are 6 apps and games to try with that new Mac\",\"link\":\"https://www.fastcompany.com/91242655/apple-computer-for-christmas-apps-games-new-mac-prince-of-persia-pixelmator-timestor-prim-werecleaner-cleanmymac\",\"snippet\":\"Got an Apple computer for the Christmas? Test out your new Mac with these games and apps, from Prince of Persia to Pixelmator to...\",\"date\":\"1 天前\",\"source\":\"Fast Company\",\"imageUrl\":\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRlx927RSBSOWjHtxw703qPbAfflLLoGQju8JDWSjPbXvqOaim6KwZZCmWtGw&s\",\"position\":6},{\"title\":\"Apple asks to participate in Google’s upcoming antitrust trial\",\"link\":\"https://siliconangle.com/2024/12/24/apple-asks-participate-googles-upcoming-antitrust-trial/\",\"snippet\":\"Apple Inc. has asked to participate in an upcoming antitrust trial that will focus on Google LLC's practices in the search market.\",\"date\":\"5 天前\",\"source\":\"SiliconANGLE\",\"imageUrl\":\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRMX3Zzl2FRtF1RIJ1GCJWCQ1oeCwMgImoMWIjhMsNuDHIqy0gMbjLzeoBfTg&s\",\"position\":7},{\"title\":\"iOS 18\",\"link\":\"https://www.apple.com/ios/ios-18/\",\"snippet\":\"iOS 18 makes iPhone even more personal, with deeper customization, new ways to connect, easier-to-find photos, and support for Apple Intelligence.\",\"date\":\"3 個月前\",\"source\":\"Apple\",\"imageUrl\":\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTRI0TomlSgsglOYP37lY_rzNQn73Q6usMWKIKFWKA4BOwYZvwnVStpyCYpBA&s\",\"position\":8},{\"title\":\"Apple's AI-Driven Growth, AirTag 2, AirPods' Health Features, And iPhone 17 Pro: This Week In Appleverse\",\"link\":\"https://www.benzinga.com/tech/24/12/42715283/apples-ai-driven-growth-airtag-2-airpods-health-features-and-iphone-17-pro-this-week-in-appleverse\",\"snippet\":\"The holiday week was abuzz with news from Apple Inc. AAPL-1.64%. Get Free Report . The tech giant continues to make headlines with its...\",\"date\":\"21 小時前\",\"source\":\"Benzinga\",\"imageUrl\":\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQW67m4S_Wfw5YAatbH-5tQeO4F7cE__L_qPAWF9sVxhJZQHMoa-Df9793ONg&s\",\"position\":9},{\"title\":\"Is Apple Inc. (AAPL) the Best Kid-friendly Stock to Buy Right Now?\",\"link\":\"https://finance.yahoo.com/news/apple-inc-aapl-best-kid-175848382.html\",\"snippet\":\"We recently compiled a list of the 10 Best Kid-Friendly Stocks To Buy Right Now. In this article, we are going to take a look at where Apple...\",\"date\":\"1 天前\",\"source\":\"Yahoo Finance\",\"imageUrl\":\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRqs3Ew-dTK28r3XdWgkrPRm-JtIU5tkqmRPSuzlUE1blx3MPEQ9CcevfvDkg&s\",\"position\":10}],\"credits\":1}\n"
     ]
    }
   ],
   "source": [
    "#利用Serper搜尋訊息，並載入\n",
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"https://google.serper.dev/news\"\n",
    "\n",
    "payload = json.dumps({\n",
    "  \"q\": \"apple inc\",\n",
    "  \"hl\": \"zh-tw\"\n",
    "})\n",
    "headers = {\n",
    "  'X-API-KEY': 'b4a777328367a49d9e0c287a72c709995c66068f',\n",
    "  'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Text Splitter文本分割器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=9,      #每個chunk大小(每幾個token切1組chunk)\n",
    "    chunk_overlap=9    #前後chunk重疊的字符\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"123456789\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['123456789']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter.split_text(text1)\n",
    "#剛好9字符沒有分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = \"123456789123456789\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['123456789',\n",
       " '234567891',\n",
       " '345678912',\n",
       " '456789123',\n",
       " '567891234',\n",
       " '678912345',\n",
       " '789123456',\n",
       " '891234567',\n",
       " '912345678',\n",
       " '123456789']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter.split_text(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "text3 = \"This is a sample text to split. It has multiple sentences\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is a',\n",
       " 'a sample',\n",
       " 'text to',\n",
       " 'split.',\n",
       " 'It has',\n",
       " 'multiple',\n",
       " 'sentence',\n",
       " 'sentences']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter.split_text(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.pdf.PyPDFLoader at 0x1687fae2520>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = PyPDFLoader(\"./data/ML-guide.pdf\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"MachineLearning-Lecture01  \\nInstructor (Andrew Ng): Okay. Good morning. Welcome to CS229, the machine \\nlearning class. So what I wanna do today is just spend a little time going over the logistics \\nof the class, and then we'll start to talk a bit about machine learning.  \\nBy way of introduction, my name's Andrew Ng and I'll be instructor for this class. And so \\nI personally work in machine learning, and I've worked on it for about 15 years now, and \\nI actually think that machine learning is the \""
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages = loader.load()\n",
    "pages[0].page_content[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=150,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]   #分割邏輯(有優先順序性)\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#分割後，總共分成幾個chunk\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"MachineLearning-Lecture01  \\nInstructor (Andrew Ng): Okay. Good morning. Welcome to CS229, the machine \\nlearning class. So what I wanna do today is just spend a little time going over the logistics \\nof the class, and then we'll start to talk a bit about machine learning.  \\nBy way of introduction, my name's Andrew Ng and I'll be instructor for this class. And so \\nI personally work in machine learning, and I've worked on it for about 15 years now, and\""
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#第1組chunk資料\n",
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I personally work in machine learning, and I've worked on it for about 15 years now, and \\nI actually think that machine learning is the most exciting field of all the computer \\nsciences. So I'm actually always excited about teaching this class. Sometimes I actually \\nthink that machine learning is not only the most exciting thing in computer science, but \\nthe most exciting thing in all of human endeavor, so maybe a little bias there.\""
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#第2組chunk資料\n",
    "docs[1].page_content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.4 Chunking分塊大小怎麼決定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import ReadTheDocsLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#讀取html檔案，可把html的語法濾掉\n",
    "loader = ReadTheDocsLoader(\n",
    "    \"htmldocs\",   #可讀取該目錄下所有html文檔\n",
    "    encoding=\"utf-8\",  # 確保編碼與文件一致\n",
    "    errors=\"ignore\"    # 忽略無法解碼的字符\n",
    ")     \n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain.indexes.vectorstore.VectorstoreIndexCreator¶\n",
      "class langchain.indexes.vectorstore.VectorstoreIndexCreator[source]¶\n",
      "Bases: BaseModel\n",
      "Logic for creating indexes.\n",
      "Create a new model by parsing and validating input data from keyword arguments.\n",
      "Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "param embedding: Embeddings [Optional]¶\n",
      "param text_splitter: TextSplitter [Optional]¶\n",
      "param vectorstore_cls: Type[VectorStore] = <class 'langchain_community.vectorstores.\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt-3.5-turbo 4096 tokens\n",
    "# If 4096 - (Input(Instruction + query + context) + output)\n",
    "#     If Chunk nums = 5:\n",
    "#         Chunk Size = 2000 / 5 = 400      *假設Instruction + query使用2000 tokens\n",
    "\n",
    "# So Chunk Size <= 400\n",
    "\n",
    "# Too small not meaningful\n",
    "# Too big not efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Encoding 'cl100k_base'>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_count(text):\n",
    "    tokens = tokenizer.encode(\n",
    "        text,\n",
    "        disallowed_special=()\n",
    "    )\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1538, 1605]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [token_count(doc.page_content) for doc in docs]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=20,\n",
    "    length_function=token_count,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = text_splitter.split_text(docs[0].page_content)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(383, 373, 345, 376, 105)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_count(chunks[0]), token_count(chunks[1]), token_count(chunks[2]), token_count(chunks[3]), token_count(chunks[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"langchain.indexes.vectorstore.VectorstoreIndexCreator¶\\nclass langchain.indexes.vectorstore.VectorstoreIndexCreator[source]¶\\nBases: BaseModel\\nLogic for creating indexes.\\nCreate a new model by parsing and validating input data from keyword arguments.\\nRaises ValidationError if the input data cannot be parsed to form a valid model.\\nparam embedding: Embeddings [Optional]¶\\nparam text_splitter: TextSplitter [Optional]¶\\nparam vectorstore_cls: Type[VectorStore] = <class 'langchain_community.vectorstores.inmemory.InMemoryVectorStore'>¶\\nparam vectorstore_kwargs: dict [Optional]¶\\nasync afrom_documents(documents: List[Document]) → VectorStoreIndexWrapper[source]¶\\nCreate a vectorstore index from documents.\\nParameters\\ndocuments (List[Document]) – \\nReturn type\\nVectorStoreIndexWrapper\\nasync afrom_loaders(loaders: List[BaseLoader]) → VectorStoreIndexWrapper[source]¶\\nCreate a vectorstore index from loaders.\\nParameters\\nloaders (List[BaseLoader]) – \\nReturn type\\nVectorStoreIndexWrapper\\nclassmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model¶\\nCreates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\\nDefault values are respected, but no other validation is performed.\\nBehaves as if Config.extra = ‘allow’ was set since it adds all passed values\\nParameters\\n_fields_set (Optional[SetStr]) – \\nvalues (Any) – \\nReturn type\\nModel\\ncopy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model¶\",\n",
       " 'Duplicate a model, optionally choose which fields to include, exclude and change.\\nParameters\\ninclude (Optional[Union[AbstractSetIntStr, MappingIntStrAny]]) – fields to include in new model\\nexclude (Optional[Union[AbstractSetIntStr, MappingIntStrAny]]) – fields to exclude from new model, as with values this takes precedence over include\\nupdate (Optional[DictStrAny]) – values to change/add in the new model. Note: the data is not validated before creating\\nthe new model: you should trust this data\\ndeep (bool) – set to True to make a deep copy of the model\\nself (Model) – \\nReturns\\nnew model instance\\nReturn type\\nModel\\ndict(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False) → DictStrAny¶\\nGenerate a dictionary representation of the model, optionally specifying which fields to include or exclude.\\nParameters\\ninclude (Optional[Union[AbstractSetIntStr, MappingIntStrAny]]) – \\nexclude (Optional[Union[AbstractSetIntStr, MappingIntStrAny]]) – \\nby_alias (bool) – \\nskip_defaults (Optional[bool]) – \\nexclude_unset (bool) – \\nexclude_defaults (bool) – \\nexclude_none (bool) – \\nReturn type\\nDictStrAny\\nfrom_documents(documents: List[Document]) → VectorStoreIndexWrapper[source]¶\\nCreate a vectorstore index from documents.\\nParameters\\ndocuments (List[Document]) – \\nReturn type\\nVectorStoreIndexWrapper',\n",
       " 'Parameters\\ndocuments (List[Document]) – \\nReturn type\\nVectorStoreIndexWrapper\\nfrom_loaders(loaders: List[BaseLoader]) → VectorStoreIndexWrapper[source]¶\\nCreate a vectorstore index from loaders.\\nParameters\\nloaders (List[BaseLoader]) – \\nReturn type\\nVectorStoreIndexWrapper\\nclassmethod from_orm(obj: Any) → Model¶\\nParameters\\nobj (Any) – \\nReturn type\\nModel\\njson(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode¶\\nGenerate a JSON representation of the model, include and exclude arguments as per dict().\\nencoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().\\nParameters\\ninclude (Optional[Union[AbstractSetIntStr, MappingIntStrAny]]) – \\nexclude (Optional[Union[AbstractSetIntStr, MappingIntStrAny]]) – \\nby_alias (bool) – \\nskip_defaults (Optional[bool]) – \\nexclude_unset (bool) – \\nexclude_defaults (bool) – \\nexclude_none (bool) – \\nencoder (Optional[Callable[[Any], Any]]) – \\nmodels_as_dict (bool) – \\ndumps_kwargs (Any) – \\nReturn type\\nunicode',\n",
       " \"dumps_kwargs (Any) – \\nReturn type\\nunicode\\nclassmethod parse_file(path: Union[str, Path], *, content_type: unicode = None, encoding: unicode = 'utf8', proto: Protocol = None, allow_pickle: bool = False) → Model¶\\nParameters\\npath (Union[str, Path]) – \\ncontent_type (unicode) – \\nencoding (unicode) – \\nproto (Protocol) – \\nallow_pickle (bool) – \\nReturn type\\nModel\\nclassmethod parse_obj(obj: Any) → Model¶\\nParameters\\nobj (Any) – \\nReturn type\\nModel\\nclassmethod parse_raw(b: Union[str, bytes], *, content_type: unicode = None, encoding: unicode = 'utf8', proto: Protocol = None, allow_pickle: bool = False) → Model¶\\nParameters\\nb (Union[str, bytes]) – \\ncontent_type (unicode) – \\nencoding (unicode) – \\nproto (Protocol) – \\nallow_pickle (bool) – \\nReturn type\\nModel\\nclassmethod schema(by_alias: bool = True, ref_template: unicode = '#/definitions/{model}') → DictStrAny¶\\nParameters\\nby_alias (bool) – \\nref_template (unicode) – \\nReturn type\\nDictStrAny\\nclassmethod schema_json(*, by_alias: bool = True, ref_template: unicode = '#/definitions/{model}', **dumps_kwargs: Any) → unicode¶\\nParameters\\nby_alias (bool) – \\nref_template (unicode) – \\ndumps_kwargs (Any) – \\nReturn type\\nunicode\\nclassmethod update_forward_refs(**localns: Any) → None¶\\nTry to update ForwardRefs on fields based on this Model, globalns and localns.\\nParameters\\nlocalns (Any) – \\nReturn type\\nNone\\nclassmethod validate(value: Any) → Model¶\\nParameters\",\n",
       " 'Return type\\nNone\\nclassmethod validate(value: Any) → Model¶\\nParameters\\nvalue (Any) – \\nReturn type\\nModel\\nExamples using VectorstoreIndexCreator¶\\nCreate a vectorstore retriever from the loader\\nRetrievers\\nSet env var OPENAI_API_KEY or load from a .env file:\\napify.md\\napify_dataset.md\\nhugging_face_dataset.md\\nimage_captions.md\\nsee https://python.langchain.com/en/latest/modules/data_connection/getting_started.html for more details']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1 Embedding與Chroma向量數據庫的創建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(\n",
    "    base_url=os.environ[\"EMBEDDINGS_BASE_URL\"]    #中介網址需加v1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"I like cats.\"\n",
    "sentence2 = \"I like dogs.\"\n",
    "sentence3 = \"The weather is ugly outside.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding1 = embeddings.embed_query(sentence1)\n",
    "embedding2 = embeddings.embed_query(sentence2)\n",
    "embedding3 = embeddings.embed_query(sentence3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9177212170497013)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(embedding1, embedding2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.7495333196158077)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(embedding1, embedding3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.7547150834376438)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(embedding2, embedding3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = \"./db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'rm' ���O�����Υ~���R�O�B�i���檺�{���Χ妸�ɡC\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#將chunks轉成documents格式\n",
    "doc_chunks = text_splitter.create_documents(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#建立向量資料庫(透過documents格式)\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=doc_chunks, \n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#建立向量資料庫(透過text格式)\n",
    "vectordb = Chroma.from_texts(\n",
    "    texts=chunks, \n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
